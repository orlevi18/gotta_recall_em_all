{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import math\n",
    "import codecs\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build index\n",
    "INDEX_NAME=\"ecom2019\"\n",
    "QUERY_TO_CATEGORY_INDEX_NAME=\"ecom2019_domcat\"\n",
    "TYPE_NAME=\"listing\"\n",
    "ID_FIELD=\"id\"\n",
    "FILES_ROOT_PATH = 'C:/Users/cpieterse/OneDrive - eBay Inc/High accuracy recall/'\n",
    "MAX_SIZE_TO_INDEX = 10000000\n",
    "BULK_SIZE = 1000\n",
    "DO_REINDEX = True # Should we reinsert all documents\n",
    "ES_VERSION = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab test mijn token ammo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import unidecode\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = SnowballStemmer(\"english\")\n",
    "TOKENIZER = RegexpTokenizer('[a-zA-Z0-9]+[a-zA-Z0-9-/]?[a-zA-Z0-9]+|[\\d\\.]+|[a-z.]+')\n",
    "\n",
    "\n",
    "def remove_periods_in_acronyms(x):\n",
    "    \"\"\"Remove the . in acronyms like d.c. or u.s.a -> dc / usa \"\"\"\n",
    "    if re.match('^([a-z][.])+[a-z]?$', x):    \n",
    "        return x.replace('.', '')\n",
    "    return x\n",
    "\n",
    "def remove_hyphens_in_words(x):\n",
    "    \"\"\"Remove the hyphen in the middel of a word, like ka-zar -> kazar, x-men -> xmen\"\"\"\n",
    "    if re.match('^[a-z]+[-][a-z]+$', x):    \n",
    "        return x.replace('-', '')\n",
    "    return x\n",
    "\n",
    "    \n",
    "def tokenize_and_strip_accents(x, do_stemming=True):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence and remove stopwords. \n",
    "    Optionally: apply phrase detection with the supplied model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        norm_string = ' ' + unidecode.unidecode(x).lower() + ' '\n",
    "    except AttributeError as e:\n",
    "        norm_string = ' ' + str(x) + ' '\n",
    "        \n",
    "    \n",
    "    #tokenizer = TreebankWordTokenizer()\n",
    "    words = TOKENIZER.tokenize(norm_string)\n",
    "    filtered_words = [word for word in words if word not in STOPWORDS]\n",
    "    if do_stemming:\n",
    "        filtered_words = [STEMMER.stem(word) for word in filtered_words]\n",
    "        \n",
    "    filtered_words = [remove_periods_in_acronyms(word) for word in filtered_words]\n",
    "    filtered_words = [remove_hyphens_in_words(word) for word in filtered_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "tokenize_and_strip_accents(\"a.b. test mijn toke-nizer a-mmo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1 52 53 hand cut'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_strip_accents(\"5.1,52,53 hand cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a > b > c a > b a\n"
     ]
    }
   ],
   "source": [
    "def trim_path_to_L(path, n):\n",
    "    split_path = path.split(' > ')\n",
    "    return ' > '.join(split_path[:n])\n",
    "    \n",
    "def dynamic_split_path(path):\n",
    "    split_path = path.split(' > ')\n",
    "    n = math.floor(len(split_path)/2)+1\n",
    "    return ' > '.join(split_path[:n])\n",
    "\n",
    "def L_minus1_split_path(path):\n",
    "    split_path = path.split(' > ')\n",
    "    n = len(split_path)-1\n",
    "    return ' > '.join(split_path[:n])\n",
    "\n",
    "print(trim_path_to_L(\"a > b > c > d\", 1), dynamic_split_path(\"a > b > c > d\"), L_minus1_split_path(\"a > b > c\"), \" > \".join(\"a > b\".split(' > ')[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_REINDEX:\n",
    "    raise ValueError(\"Please skip the index creation part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cpieterse\\\\Documents\\\\GitHub\\\\ecom2019\\\\a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.path.join(os.getcwd(), 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_properties = {\n",
    "       \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"my_analyzer\",\n",
    "                \"fields\": {\n",
    "                    \"title_shingles\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"my_shingle_analyzer\",\n",
    "                        \"search_analyzer\": \"my_stemmer_analyzer\"\n",
    "                    }, \n",
    "                    \"title_stems\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"my_stemmer_analyzer\",\n",
    "                        \"search_analyzer\": \"my_stemmer_analyzer\"\n",
    "                    },\n",
    "                    \"title_tokens\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"my_analyzer\",\n",
    "                        \"search_analyzer\": \"my_analyzer\"\n",
    "                    }\n",
    "\n",
    "                }\n",
    "            }, \n",
    "            \"title_customized\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"my_SB_stemmer_analyzer\",\n",
    "                \"search_analyzer\": \"my_SB_stemmer_analyzer\",\n",
    "                \"fields\": {\n",
    "                    \"stems\":{\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"my_SB_stemmer_analyzer\",\n",
    "                        \"search_analyzer\": \"my_SB_stemmer_analyzer\"                        \n",
    "                    }\n",
    "                }\n",
    "\n",
    "            },\n",
    "            \"category\": {\n",
    "                \"type\": \"keyword\",\n",
    "            },\n",
    "#            \"category_L1\": {\n",
    "#                \"type\": \"keyword\",\n",
    "#            },\n",
    "#            \"category_L2\": {\n",
    "#                \"type\": \"keyword\",\n",
    "#            },\n",
    "#            \"category_L3\": {\n",
    "#                \"type\": \"keyword\",\n",
    "#            },\n",
    "#            \"category_L4\": {\n",
    "#                \"type\": \"keyword\",\n",
    "#            },\n",
    "#            \"category_L5\": {\n",
    "#                \"type\": \"keyword\",\n",
    "#            },\n",
    "#            \"category_L-1\": {\n",
    "#                \"type\": \"keyword\",\n",
    "#            },\n",
    "#            \"category_L~\": {\n",
    "#                \"type\": \"keyword\",\n",
    "#            },\n",
    "            \"title_category\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"shingles\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"my_shingle_analyzer\",\n",
    "                        \"search_analyzer\": \"my_stemmer_analyzer\"\n",
    "                    },\n",
    "                    \"stems\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"my_stemmer_analyzer\",\n",
    "                        \"search_analyzer\": \"my_stemmer_analyzer\"\n",
    "                    }\n",
    "\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "if ES_VERSION < 7:\n",
    "    listing_properties = {TYPE_NAME: listing_properties}\n",
    "\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "          \"similarity\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"BM25\",\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"my_shingle_filter\": {\n",
    "                    \"type\": \"shingle\",\n",
    "                    \"output_unigrams\": False\n",
    "                },\n",
    "                \"synonym_filter\" : {\n",
    "                        \"type\" : \"synonym\",\n",
    "                        \"lenient\": True,\n",
    "                        #\"synonyms\" : [\"ka-zar,kazar\"]\n",
    "                        \"synonyms_path\" : \"analysis/synonyms.txt\"\n",
    "                },\n",
    "                \"snowball_english\" : {\n",
    "                    \"type\" : \"snowball\",\n",
    "                    \"language\" : \"English\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"my_shingle_analyzer\": {\n",
    "                    \"type\":\t\"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"synonym_filter\",\n",
    "                        \"my_shingle_filter\",\n",
    "                        \"porter_stem\"\n",
    "                    ]\n",
    "                },\n",
    "                \"my_stemmer_analyzer\": {\n",
    "                    \"type\":\t\"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"synonym_filter\",\n",
    "                        \"porter_stem\"\n",
    "                    ]\n",
    "                },\n",
    "                \"my_SB_stemmer_analyzer\": {\n",
    "                    \"type\":\t\"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"synonym_filter\",\n",
    "                        \"snowball_english\"\n",
    "                    ]\n",
    "                },\n",
    "                \"my_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"synonym_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": listing_properties\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting 'ecom2019' index...\n",
      " response: '{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ecom2019'}'\n"
     ]
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    print(\"deleting '%s' index...\" % (INDEX_NAME))\n",
    "    res = es.indices.delete(index = INDEX_NAME)\n",
    "    \n",
    "res = es.indices.create(index=INDEX_NAME, body=mapping)\n",
    "print(\" response: '%s'\" % (res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulk index 10000\n",
      "bulk index 20000\n",
      "bulk index 30000\n",
      "bulk index 40000\n",
      "ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10))\n",
      "bulk index 40000\n",
      "bulk index 50000\n",
      "bulk index 60000\n",
      "bulk index 70000\n",
      "bulk index 80000\n",
      "bulk index 90000\n",
      "bulk index 100000\n",
      "bulk index 110000\n",
      "bulk index 120000\n",
      "bulk index 130000\n",
      "bulk index 140000\n",
      "bulk index 150000\n",
      "bulk index 160000\n",
      "bulk index 170000\n",
      "bulk index 180000\n",
      "bulk index 190000\n",
      "bulk index 200000\n",
      "bulk index 210000\n",
      "bulk index 220000\n",
      "bulk index 230000\n",
      "bulk index 240000\n",
      "bulk index 250000\n",
      "bulk index 260000\n",
      "bulk index 270000\n",
      "bulk index 280000\n",
      "bulk index 290000\n",
      "bulk index 300000\n",
      "bulk index 310000\n",
      "bulk index 320000\n",
      "bulk index 330000\n",
      "bulk index 340000\n",
      "bulk index 350000\n",
      "bulk index 360000\n",
      "bulk index 370000\n",
      "bulk index 380000\n",
      "bulk index 390000\n",
      "bulk index 400000\n",
      "bulk index 410000\n",
      "bulk index 420000\n",
      "bulk index 430000\n",
      "bulk index 440000\n",
      "bulk index 450000\n",
      "bulk index 460000\n",
      "bulk index 470000\n",
      "bulk index 480000\n",
      "bulk index 490000\n",
      "bulk index 500000\n",
      "bulk index 510000\n",
      "bulk index 520000\n",
      "bulk index 530000\n",
      "bulk index 540000\n",
      "bulk index 550000\n",
      "bulk index 560000\n",
      "bulk index 570000\n",
      "bulk index 580000\n",
      "bulk index 590000\n",
      "bulk index 600000\n",
      "bulk index 610000\n",
      "bulk index 620000\n",
      "bulk index 630000\n",
      "bulk index 640000\n",
      "bulk index 650000\n",
      "bulk index 660000\n",
      "bulk index 670000\n",
      "bulk index 680000\n",
      "bulk index 690000\n",
      "bulk index 700000\n",
      "bulk index 710000\n",
      "bulk index 720000\n",
      "bulk index 730000\n",
      "ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10))\n",
      "bulk index 730000\n",
      "bulk index 740000\n",
      "bulk index 750000\n",
      "bulk index 760000\n",
      "bulk index 770000\n",
      "bulk index 780000\n",
      "bulk index 790000\n",
      "bulk index 800000\n",
      "bulk index 810000\n",
      "bulk index 820000\n",
      "bulk index 830000\n",
      "bulk index 840000\n",
      "bulk index 850000\n",
      "bulk index 860000\n",
      "bulk index 870000\n",
      "bulk index 879288\n"
     ]
    }
   ],
   "source": [
    "# read csv file as unicode\n",
    "from elasticsearch import NotFoundError\n",
    "from time import sleep\n",
    "\n",
    "dumpfile=codecs.open(FILES_ROOT_PATH+\"documents.tsv\",\"r\",encoding='utf=8')\n",
    "reader=csv.reader(dumpfile,delimiter='\\t')\n",
    "next(reader, None)  # skip the headers\n",
    "BULK_SIZE = 10000\n",
    "# index in batches\n",
    "bulk_data = [] \n",
    "i=1\n",
    "listings_failed_to_index=open(FILES_ROOT_PATH+\"listings_failed_to_index.txt\",\"w\")\n",
    "for line in reader:\n",
    "    try:\n",
    "        es.get(index=INDEX_NAME\n",
    "               , doc_type=TYPE_NAME if ES_VERSION<7 else None\n",
    "               , id=line[0])\n",
    "        if i % 1000 == 0: \n",
    "            print(\"Skipped until\", i)\n",
    "        i=i+1\n",
    "        continue\n",
    "    except NotFoundError as e:\n",
    "        pass\n",
    "    \n",
    "    data_dict = {}\n",
    "    data_dict['id']=line[0]\n",
    "    data_dict['title']=line[2]\n",
    "    data_dict['title_customized']=tokenize_and_strip_accents(line[2], do_stemming=False)\n",
    "    data_dict['category']=line[3].lower()\n",
    "#    data_dict['category_L1']=trim_path_to_L(line[3].lower(), 1)\n",
    "#    data_dict['category_L2']=trim_path_to_L(line[3].lower(), 2)\n",
    "#    data_dict['category_L3']=trim_path_to_L(line[3].lower(), 3)\n",
    "#    data_dict['category_L4']=trim_path_to_L(line[3].lower(), 4)\n",
    "#    data_dict['category_L5']=trim_path_to_L(line[3].lower(), 5)\n",
    "#    data_dict['category_L-1']=L_minus1_split_path(line[3].lower())\n",
    "#    data_dict['category_L~']=dynamic_split_path(line[3].lower())\n",
    "    data_dict['title_category'] = data_dict['title'] + ' ' + data_dict['category']\n",
    "\n",
    "    op_dict = {\n",
    "        \"index\": {\n",
    "            \"_index\": INDEX_NAME, \n",
    "            \"_id\": line[0]\n",
    "            # , \"_source\": data_dict\n",
    "        }\n",
    "    }\n",
    "    if ES_VERSION < 7:\n",
    "        op_dict[\"index\"].update({\"_type\": TYPE_NAME})\n",
    "        \n",
    "    #es.index(index=INDEX_NAME, id=line[0], body=data_dict)\n",
    "\n",
    "    # op_dict.update(data_dict)\n",
    "    bulk_data.append(op_dict)\n",
    "    bulk_data.append(data_dict)\n",
    "        \n",
    "    if i%BULK_SIZE==0:\n",
    "        print(\"bulk index \"+str(i))\n",
    "        try:\n",
    "            res = es.bulk(index = INDEX_NAME, body = bulk_data, refresh = True)\n",
    "            # res = 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            listings_failed_to_index.write(line[0]+\"\\n\")\n",
    "            i -= BULK_SIZE\n",
    "            sleep(2)\n",
    "                \n",
    "        bulk_data = []\n",
    "    \n",
    "    if i >= MAX_SIZE_TO_INDEX:\n",
    "        break        \n",
    "            \n",
    "    i=i+1\n",
    "\n",
    "\n",
    "if len(bulk_data)>0:\n",
    "    print(\"bulk index \"+str(i))\n",
    "    res = es.bulk(index = INDEX_NAME, body = bulk_data, refresh = True)\n",
    "\n",
    "dumpfile.close()\n",
    "listings_failed_to_index.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_index': 'ecom2019', '_type': 'listing', '_id': '1733264', '_version': 1, '_seq_no': 146369, '_primary_term': 1, 'found': True, '_source': {'id': '1733264', 'title': 'Comic  KAZAR #1', 'title_customized': 'comic kazar 1', 'category': ' collectibles > comics > collections', 'title_category': 'Comic  KAZAR #1  collectibles > comics > collections'}}\n"
     ]
    }
   ],
   "source": [
    "id = 1733264\n",
    "try:\n",
    "    if ES_VERSION < 7:\n",
    "        print(es.get(index=INDEX_NAME, doc_type=TYPE_NAME, id=id))\n",
    "    else:\n",
    "        print(es.get(index=INDEX_NAME, id=id))\n",
    "except NotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the DomCat mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "          \"similarity\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"BM25\",\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"my_shingle_filter\": {\n",
    "                    \"type\": \"shingle\",\n",
    "                    \"output_unigrams\": False\n",
    "                },\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"my_shingle_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"my_shingle_filter\",\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "       \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"shingles\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"my_shingle_analyzer\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"category_level\": {\n",
    "                \"type\": \"keyword\",\n",
    "            },\n",
    "            \"category\": {\n",
    "                \"type\": \"keyword\",\n",
    "            },\n",
    "           \"score\": {\n",
    "               \"type\": \"float\"\n",
    "           }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestError",
     "evalue": "RequestError(400, 'mapper_parsing_exception', 'Root mapping definition has unsupported parameters:  [category_level : {type=keyword}] [score : {type=float}] [query : {type=text, fields={shingles={analyzer=my_shingle_analyzer, type=text}}}] [category : {type=keyword}]')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d07491ad4cbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQUERY_TO_CATEGORY_INDEX_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mQUERY_TO_CATEGORY_INDEX_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" response: '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\client\\utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\client\\indices.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, index, body, params)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Empty value passed for a required argument 'index'.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         return self.transport.perform_request(\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[1;34m\"PUT\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         )\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[0;32m    351\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                     \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m                 )\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\connection\\http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             )\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         self.log_request_success(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\connection\\base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[1;34m(self, status_code, raw_data)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         )\n",
      "\u001b[1;31mRequestError\u001b[0m: RequestError(400, 'mapper_parsing_exception', 'Root mapping definition has unsupported parameters:  [category_level : {type=keyword}] [score : {type=float}] [query : {type=text, fields={shingles={analyzer=my_shingle_analyzer, type=text}}}] [category : {type=keyword}]')"
     ]
    }
   ],
   "source": [
    "if es.indices.exists(QUERY_TO_CATEGORY_INDEX_NAME):\n",
    "    print(\"deleting '%s' index...\" % (QUERY_TO_CATEGORY_INDEX_NAME))\n",
    "    res = es.indices.delete(index = QUERY_TO_CATEGORY_INDEX_NAME)\n",
    "    \n",
    "res = es.indices.create(index=QUERY_TO_CATEGORY_INDEX_NAME, body=mapping)\n",
    "print(\" response: '%s'\" % (res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_in_es(index_name, doc_id, **kwargs):   \n",
    "    es.index(index=index_name, id=doc_id, body=kwargs)\n",
    "    for k,v in kwargs.items(): \n",
    "        print(k,v)\n",
    "\n",
    "doc_id = 0\n",
    "#for level in ['breadcrumb', 'L1', 'L2', 'L3', 'L4', 'L5', 'L-1', 'L~']:\n",
    "for level in ['L1']:\n",
    "    df = pd.read_csv(FILES_ROOT_PATH + 'queries_with_DomCat_10_' + level.replace('~', '_') + '.tsv', sep='\\t')\n",
    "    for i, r in df.iterrows():\n",
    "        print(i,doc_id = doc_id, query=r.query, level=level, category=)\n",
    "#insert_in_es(QUERY_TO_CATEGORY_INDEX_NAME, queryid=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test some queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (field_to_use, analyzer) in {\"title.title_shingles\":\"my_shingle_analyzer\"\n",
    "                                 , \"title.title_stems\":\"my_stemmer_analyzer\"\n",
    "                                 , \"title.title_tokens\":\"my_analyzer\"\n",
    "                                 , \"title_category.stems\":\"my_stemmer_analyzer\"\n",
    "                                 , \"title_customized.stems\":\"my_SB_stemmer_analyzer\"}.items():\n",
    "    query = 'hand cut'\n",
    "    \n",
    "    if field_to_use==\"title_customized.stems\":\n",
    "        query = tokenize_and_strip_accents(query, False)\n",
    "        \n",
    "    adidbody={\n",
    "            \"from\" : 0\n",
    "            , \"size\" : 10000\n",
    "            , \"query\": {\n",
    "                \"match\" : {\n",
    "                    field_to_use:{\"query\":query,\n",
    "                                  \"operator\" : \"and\", \n",
    "                                  \"analyzer\": analyzer}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    adidhit = es.search(index=INDEX_NAME\n",
    "                        #, doc_type=TYPE_NAME if ES_VERSION<7 else None\n",
    "                        , body=adidbody)\n",
    "    print(field_to_use, adidhit['hits']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_strip_accents(query, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FIELDS =  {\"title.title_shingles\":\"my_shingle_analyzer\"\n",
    "                 , \"title.title_stems\":\"my_stemmer_analyzer\"\n",
    "                 , \"title.title_tokens\":\"my_analyzer\"\n",
    "                 , \"title_category.stems\":\"my_stemmer_analyzer\"\n",
    "                 , \"title_customized.stems\":\"my_SB_stemmer_analyzer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.read_csv(FILES_ROOT_PATH+\"queries.tsv\",encoding='utf=8', sep='\\t')\n",
    "#reader=csv.reader(testfile,delimiter='\\t')\n",
    "#next(reader, None)  # skip the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_results_file(filename):\n",
    "    \"\"\"\n",
    "    Open the file for writing and append the header\n",
    "    Returns file writer object\n",
    "    \"\"\"\n",
    "    \n",
    "    # similar_listings = codecs.open(\"results/similar_listings.tsv\",\"w\",'utf-8')\n",
    "    similar_listings = codecs.open(filename, \"w\",'utf-8')\n",
    "    similar_listings.write('\\t'.join(['queryid', 'adid', 'score', 'query', 'query_tokens', 'title', 'category'\n",
    "                                      # , 'category_L1', 'category_L2', 'category_L3', 'category_L4'\n",
    "                                      # , 'category_L5', 'category_L-1', 'category_L~'\n",
    "                                     ]))\n",
    "    similar_listings.write('\\n')\n",
    "    return similar_listings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string_output(str_value, sep='\\t'):\n",
    "    \"\"\"\n",
    "    Enclose a string with quotes and replace special characters that will break the file\n",
    "    Retuns: enclosed and escaped string\n",
    "    \"\"\"\n",
    "    return '\"' + (str_value\n",
    "                  .replace(sep, '_')\n",
    "                  .replace('\\n', '_')\n",
    "                  .replace('\"', '_')) + '\"'\n",
    "    \n",
    "to_string_output('a'), to_string_output('a\\tb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_es_query(query, field_to_use, analyzer):\n",
    "        return {\n",
    "            \"from\" : 0, \n",
    "            \"size\" : 10000, \n",
    "            \"query\": {\n",
    "                \"match\" : {\n",
    "                    field_to_use:{\n",
    "                        \"query\":query,\n",
    "                        \"operator\" : \"and\",\n",
    "                        \"analyzer\": analyzer\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (field_to_use, analyzer) in QUERY_FIELDS.items():\n",
    "    print(\"Starting\", field_to_use)\n",
    "    file_writer = open_results_file(\"results/found_ads_\" + field_to_use + \".tsv\")\n",
    "\n",
    "    for i, row  in queries.iterrows():\n",
    "        queryid = row.query_id\n",
    "        query = row.query.lower()\n",
    "        if field_to_use==\"title_customized.stems\":\n",
    "            query = tokenize_and_strip_accents(query, False)\n",
    "\n",
    "        #adcat=line[2]\n",
    "        adidbody = construct_es_query(query, field_to_use, analyzer)\n",
    "        if queryid % 10 == 0:\n",
    "            print(queryid)\n",
    "\n",
    "        page = es.search(index=INDEX_NAME, body=adidbody, scroll='1m')\n",
    "        sid = page['_scroll_id']\n",
    "        page_size=len(page['hits']['hits'])\n",
    "        while page_size>0:        \n",
    "            for hit in page['hits']['hits']:\n",
    "                simadid=hit[\"_id\"]\n",
    "                simtitle=hit[\"_source\"]['title']\n",
    "                score=hit[\"_score\"]\n",
    "                try:\n",
    "                    # similar_listings.write(str(queryid)+\"\\t\"+str(simadid)+\"\\t\"+str(score)+\"\\t\"+str(hit[\"_source\"])+\"\\n\")\n",
    "                    file_writer.write('\\t'.join([str(queryid)                                             \n",
    "                                                      , str(simadid)\n",
    "                                                      , str(score)\n",
    "                                                      , to_string_output(query)\n",
    "                                                      , str(len(query.split(' ')))\n",
    "                                                      , to_string_output(hit['_source']['category'])\n",
    "                                                      , to_string_output(hit['_source']['title'])\n",
    "                                                      #, to_string_output(hit['_source']['category_L1'])\n",
    "                                                      #, to_string_output(hit['_source']['category_L2'])\n",
    "                                                      #, to_string_output(hit['_source']['category_L3'])\n",
    "                                                      #, to_string_output(hit['_source']['category_L4'])\n",
    "                                                      #, to_string_output(hit['_source']['category_L5'])\n",
    "                                                      #, to_string_output(hit['_source']['category_L-1'])\n",
    "                                                      #, to_string_output(hit['_source']['category_L~'])\n",
    "                                                ]))\n",
    "                    file_writer.write('\\n')\n",
    "\n",
    "                    # print(str(queryid)+\"\\t\"+str(simadid)+\"\\t\"+str(score)+\"\\t\"+str(hit[\"_source\"])+\"\\n\")\n",
    "                except Exception as e:\n",
    "                    #TODO check for file writer related exception only\n",
    "                    print(e)\n",
    "            \n",
    "            # Next page\n",
    "            page = es.scroll(scroll_id=sid, scroll = '1m')\n",
    "            sid = page['_scroll_id']\n",
    "            page_size=len(page['hits']['hits'])\n",
    "\n",
    "\n",
    "    file_writer.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
